{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### **Installing Dependencies**\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "BZXOBuy9NLav"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkMHD6ozLhXE",
    "outputId": "5bb1e6ed-6733-4f5e-cef1-bffdc34b821e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m16.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.5/76.5 kB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m469.0/469.0 kB\u001B[0m \u001B[31m28.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.4/179.4 kB\u001B[0m \u001B[31m16.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m52.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.0/40.0 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.5/110.5 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m302.0/302.0 kB\u001B[0m \u001B[31m17.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m300.4/300.4 kB\u001B[0m \u001B[31m24.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.3/134.3 kB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.0.292 \\\n",
    "    openai==0.28.0 \\\n",
    "    datasets==2.10.1 \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating Chatbot\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "h1NbtXtDNTdX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FGgU4V9sMhVp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API\"\n",
    "API_KEY_ENV_NAME = \"OPENAI_API_KEY\"\n",
    "MODEL_NAME = 'gpt-3.5-turbo'\n",
    "DEFAULT_API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "api_key = os.getenv(API_KEY_ENV_NAME, DEFAULT_API_KEY)\n",
    "os.environ[SECRET_KEY] = api_key\n",
    "chat = ChatOpenAI(openai_api_key=api_key, model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt engineering structure of GPT\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand gravity constant.\"}\n",
    "]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TM50Yfk7NnII",
    "outputId": "52ff0a16-ac00-484f-ecab-7a168c301fc0"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user', 'content': 'Hi, how are you today?'},\n",
       " {'role': 'assistant', 'content': \"I'm great thank you. How can I help you?\"},\n",
       " {'role': 'user', 'content': \"I'd like to understand gravity constant.\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt structure of langchain\n",
    "\n",
    "from langchain.schema import(\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "message = [\n",
    "\n",
    "           SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "           HumanMessage(content= \"Hi, how are you today?\"),\n",
    "           AIMessage(content= \"I am great thank you. How can I help you?\"),\n",
    "           HumanMessage(content = \"I would like to understand gravity constant.\" )\n",
    "\n",
    "\n",
    "\n",
    "]"
   ],
   "metadata": {
    "id": "KUm7v1YAPJVs"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Current Question in Langchain:\n",
    "\n",
    "**I would like to understand gravity constant?**"
   ],
   "metadata": {
    "id": "TboFp34t28YV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = chat(message)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ly2xqQQgQqVB",
    "outputId": "1bcfcee2-526b-4d9f-9403-3402e9f581d7"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, I can help with that. The gravitational constant, denoted as G, is a fundamental physical constant that appears in Newton's law of universal gravitation. It is used to quantify the strength of the gravitational force between two objects.\\n\\nThe value of the gravitational constant is approximately 6.67430 x 10^-11 cubic meters per kilogram per second squared (m^3/kg/s^2) in the International System of Units (SI). This means that for every kilogram of mass, the force of gravity between two objects separated by a distance of one meter will be approximately 6.67430 x 10^-11 newtons.\\n\\nThe gravitational constant plays a crucial role in understanding the behavior of celestial bodies, such as planets, stars, and galaxies. It helps us calculate the force of gravity between objects, determine the weight of an object on a planet's surface, and even understand the dynamics of the universe on a larger scale.\\n\\nI hope this gives you a basic understanding of the gravitational constant. Let me know if you have any more questions!\", additional_kwargs={}, example=False)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(response.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOBnqcHvRF8L",
    "outputId": "7730d315-4e25-45fa-c4ec-973ba2dfebec"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sure, I can help with that. The gravitational constant, denoted as G, is a fundamental physical constant that appears in Newton's law of universal gravitation. It is used to quantify the strength of the gravitational force between two objects.\n",
      "\n",
      "The value of the gravitational constant is approximately 6.67430 x 10^-11 cubic meters per kilogram per second squared (m^3/kg/s^2) in the International System of Units (SI). This means that for every kilogram of mass, the force of gravity between two objects separated by a distance of one meter will be approximately 6.67430 x 10^-11 newtons.\n",
      "\n",
      "The gravitational constant plays a crucial role in understanding the behavior of celestial bodies, such as planets, stars, and galaxies. It helps us calculate the force of gravity between objects, determine the weight of an object on a planet's surface, and even understand the dynamics of the universe on a larger scale.\n",
      "\n",
      "I hope this gives you a basic understanding of the gravitational constant. Let me know if you have any more questions!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " *Appending the message, to create a message on the top of it.*"
   ],
   "metadata": {
    "id": "0e773cfxUPm3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "message.append(response)\n",
    "\n",
    "#new prompt\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content = \"Does it can vary by the place on earth?\"\n",
    ")\n",
    "\n",
    "message.append(prompt)"
   ],
   "metadata": {
    "id": "bCh6KMO7UKuy"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = chat(message)\n",
    "print(response.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zy1sacWgU9yM",
    "outputId": "5d602c21-47b0-4587-95ef-f0a9f6d45380"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No, the gravitational constant, G, does not vary by location on Earth. It is a universal constant, meaning its value is the same everywhere in the universe. \n",
      "\n",
      "However, please note that the acceleration due to gravity, which is related to the gravitational constant, can vary slightly depending on the location on Earth. This is because the acceleration due to gravity is influenced by factors such as the Earth's rotation, shape, and local variations in density. These factors can cause slight variations in the strength of gravity from one location to another on Earth's surface.\n",
      "\n",
      "For practical purposes, the standard value of the acceleration due to gravity on Earth is usually taken as approximately 9.8 meters per second squared (m/s^2). However, in reality, it can vary slightly, such as at the equator where the centrifugal force due to the Earth's rotation causes a slight reduction in gravity.\n",
      "\n",
      "So, while the gravitational constant itself is constant, the acceleration due to gravity can vary slightly depending on the location on Earth.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem??"
   ],
   "metadata": {
    "id": "n-8W6aZ2VNk3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "message.append(response)\n",
    "\n",
    "#new prompt: something gpt doesn't know\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content = \"Can you tell me about the LLMChain in LangChain?\"\n",
    ")\n",
    "\n",
    "message.append(prompt)"
   ],
   "metadata": {
    "id": "2DeyZNWRVEMs"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = chat(message)\n",
    "print(response.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDsn3JM_Vy3L",
    "outputId": "a381ef1c-38e8-4812-f8da-386f973e9cdb"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I'm sorry, but I am not familiar with a specific technology called LLMChain in LangChain. It's possible that the term you mentioned is specific to a certain context or project that I am not aware of. \n",
      "\n",
      "If you can provide more information or context about LLMChain and LangChain, I might be able to assist you further.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Model failed to answer, lets give it some knowledge*"
   ],
   "metadata": {
    "id": "XKx6-5kj4Ftx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ],
   "metadata": {
    "id": "hHG_f3yCV53B"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Adding the source_knowledge, and defining query*"
   ],
   "metadata": {
    "id": "EAxEkF-mW3AJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\""
   ],
   "metadata": {
    "id": "Kxx8u2VHW2uJ"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "aug_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:{source_knowledge}\n",
    "Query: {query}\"\"\""
   ],
   "metadata": {
    "id": "vuGdGu0JWx_D"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = HumanMessage(content = aug_prompt)\n",
    "message.append(prompt)"
   ],
   "metadata": {
    "id": "zaAIAEAzXd1r"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = chat(message)\n",
    "print(response.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bEvMwzMXsDX",
    "outputId": "0a47739d-d782-4e82-9657-cb35492f3f60"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The LLMChain is a common type of chain within the LangChain framework. It is used to connect a PromptTemplate, a language model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables and uses the PromptTemplate to format them into a prompt. The prompt is then passed to the language model for processing. Finally, the output of the language model is parsed into a final format using the provided OutputParser, if available.\n",
      "\n",
      "In essence, the LLMChain allows for the integration of language models into the LangChain framework. It enables the development of applications that not only make use of language models through an API but also connect them to other data sources and allow for interaction with the environment. The LangChain framework aims to empower developers to create powerful and differentiated applications by combining language models with data-awareness and agency.\n",
      "\n",
      "If you have any more specific questions or need further clarification, feel free to ask!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Trying the same on a larger level**"
   ],
   "metadata": {
    "id": "JPNiZvbzX918"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "\n",
    "    split = \"train\"\n",
    "\n",
    ")"
   ],
   "metadata": {
    "id": "2MH3odOyXzC8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgELR7MBYi4k",
    "outputId": "be5ea131-202e-49e9-e255-113c1f5f64d2"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Data set is about LLamba 2 papers\n",
    "\n",
    "dataset[1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oA6hIxfJYrDQ",
    "outputId": "25cc71cd-3208-44f2-fcc6-0cd171daa45b"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '1',\n",
       " 'chunk': 'January 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but\\nrather learned in a supervised way. Our deep hierarchical architectures achieve the best\\npublished results on benchmarks for object classi\\x0ccation (NORB, CIFAR10) and handwritten\\ndigit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep\\nnets trained by simple back-propagation perform better than more shallow ones. Learning\\nis surprisingly rapid. NORB is completely trained within \\x0cve epochs. Test error rates on\\nMNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.\\n1 Introduction\\nThe human visual system e\\x0eciently recognizes and localizes objects within cluttered scenes. For\\narti\\x0ccial systems, however, this is still di\\x0ecult, due to viewpoint-dependent object variability,\\nand the high in-class variability of many object types. Deep hierarchical neural models roughly\\nmimick the nature of mammalian visual cortex, and by community consensus are among the most\\npromising architectures for such tasks. The most successful hierarchical object recognition systems\\nall extract localized features from input images, convolving image patches with \\x0clters. Filter',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. Cireşan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'Jürgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Creating Knowledge Database**"
   ],
   "metadata": {
    "id": "SAiDmcmMZCdb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pinecone\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('VDB API') \n",
    "    environment=os.environ.get('gcp-starter') or 'gcp-starter'\n",
    ")"
   ],
   "metadata": {
    "id": "kb9mr7fvYuf6"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "index_name = \"llamba-2-rag\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine'\n",
    "    )\n",
    "\n",
    "while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
    "    time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)\n"
   ],
   "metadata": {
    "id": "6_NCfKVGZjod"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "index.describe_index_stats()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjAs_c80aEtq",
    "outputId": "4e45fd4d-7087-41ec-decd-550d9797e4b7"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04838,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Creating Dense Vectors**"
   ],
   "metadata": {
    "id": "zbuKIQcLb1ix"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model = \"text-embedding-ada-002\")"
   ],
   "metadata": {
    "id": "B-l6pUyfbu1z"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "texts = [\n",
    "    \"this is the first chuck of text\",\n",
    "    \"this is the second chuck of the text is here\"\n",
    "]\n",
    "\n",
    "response = embed_model.embed_documents(texts)\n",
    "len(response), len(response[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAevHH1ycLnT",
    "outputId": "ba9200eb-617b-462c-af28-9e5fac7b7af1"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data = dataset.to_pandas()\n",
    "batch_size = 100"
   ],
   "metadata": {
    "id": "V5PUSUCPcm-F"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "\n",
    "    batch = data.iloc[i:i_end]\n",
    "    #unique ids\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a5a59994b6a2441db9b6b7abfa3f8aed",
      "f8bbb55a3c69494aa15bb55b57b0b367",
      "897e1279ce14411bbd14db04b0e55fca",
      "a4f6a641e3bc4dca98856d8e416dcec8",
      "f928c892b2a34715a0f1eb26bf78254c",
      "dae3826a4a2c4c759ab59565376aa5fd",
      "8c6d4585ac58417781c374a12ad68118",
      "2b5d617095154c5db266a45af2c3940c",
      "d122063a49f24b7bbee2b9b531fd3c4a",
      "8e61b3c7f2dd43f685e4b777d05597c9",
      "3c82478a1a094af0816d8980cb428929"
     ]
    },
    "id": "pz_czV1NcvO8",
    "outputId": "50ef4dcc-1021-4578-8e15-e7f72a63cabf"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5a59994b6a2441db9b6b7abfa3f8aed"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "index.describe_index_stats()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUB4RcbTcyBi",
    "outputId": "ea226159-69d4-4ab8-d98f-86712706d639"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04838,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Retrieval Augmented Generation**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "HbLjsOjJdRED"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "vectorestore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ],
   "metadata": {
    "id": "h1cHQ66Yc79I"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "query = \"what are the use cases of LLamba 2\"\n",
    "\n",
    "vectorestore.similarity_search(query, k=3)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dNk0WGGdwKQ",
    "outputId": "d1da2d63-ad4b-421b-a8a1-1573d9d9edb0"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='models will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Connecting our vector store to chatbot**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "BkOBnh6neTr1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorestore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ],
   "metadata": {
    "id": "iNUff4czeA_j"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(augment_prompt(query))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GC-2aIHPedQz",
    "outputId": "0948736e-c924-43d4-c44d-0ed1eafd615a"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "models will be released as we improve model safety with community feedback.\n",
      "License A custom commercial license is available at: ai.meta.com/resources/\n",
      "models-and-libraries/llama-downloads/\n",
      "Where to send commentsInstructions on how to provide feedback or comments on the model can be\n",
      "found in the model README, or by opening an issue in the GitHub repository\n",
      "(https://github.com/facebookresearch/llama/ ).\n",
      "Intended Use\n",
      "Intended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\n",
      "are intended for assistant-like chat, whereas pretrained models can be adapted\n",
      "for a variety of natural language generation tasks.\n",
      "Out-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\n",
      "compliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\n",
      "that is prohibited by the Acceptable Use Policy and Licensing Agreement for\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\n",
      "Hardware and Software (Section 2.2)\n",
      "Training Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\n",
      "\n",
      "    Query: what are the use cases of LLamba 2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "\n",
    "message.append(prompt)\n"
   ],
   "metadata": {
    "id": "Ohkrrx6hehzb"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = chat(message)\n",
    "print(response.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4MppPxbfSeN",
    "outputId": "eb302aba-a3c7-4b42-f97a-115cc2bee5d8"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Based on the provided context, the use cases of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), are primarily focused on dialogue applications. The fine-tuned LLMs in the Llama 2 collection, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for assistant-like chat scenarios. These models can be used for commercial and research purposes in English for tasks such as natural language generation.\n",
      "\n",
      "It is worth noting that the Llama 2 models have been designed to perform well on benchmarks and have shown to outperform existing open-source chat models in most cases. They may also be considered as suitable substitutes for closed-source models based on human evaluations for helpfulness and safety.\n",
      "\n",
      "However, it is important to adhere to the intended use cases and not use Llama 2 models in any way that violates applicable laws or regulations, or in languages other than English. Any usage that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2, as well as any out-of-scope uses, should be avoided.\n",
      "\n",
      "For further information and instructions on providing feedback or comments on the Llama 2 models, you can refer to the model README or open an issue in the GitHub repository provided in the context.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "r6KECEvifc5z"
   },
   "execution_count": 41,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "a5a59994b6a2441db9b6b7abfa3f8aed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8bbb55a3c69494aa15bb55b57b0b367",
       "IPY_MODEL_897e1279ce14411bbd14db04b0e55fca",
       "IPY_MODEL_a4f6a641e3bc4dca98856d8e416dcec8"
      ],
      "layout": "IPY_MODEL_f928c892b2a34715a0f1eb26bf78254c"
     }
    },
    "f8bbb55a3c69494aa15bb55b57b0b367": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dae3826a4a2c4c759ab59565376aa5fd",
      "placeholder": "​",
      "style": "IPY_MODEL_8c6d4585ac58417781c374a12ad68118",
      "value": "100%"
     }
    },
    "897e1279ce14411bbd14db04b0e55fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b5d617095154c5db266a45af2c3940c",
      "max": 49,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d122063a49f24b7bbee2b9b531fd3c4a",
      "value": 49
     }
    },
    "a4f6a641e3bc4dca98856d8e416dcec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e61b3c7f2dd43f685e4b777d05597c9",
      "placeholder": "​",
      "style": "IPY_MODEL_3c82478a1a094af0816d8980cb428929",
      "value": " 49/49 [01:24&lt;00:00,  1.50s/it]"
     }
    },
    "f928c892b2a34715a0f1eb26bf78254c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dae3826a4a2c4c759ab59565376aa5fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c6d4585ac58417781c374a12ad68118": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b5d617095154c5db266a45af2c3940c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d122063a49f24b7bbee2b9b531fd3c4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8e61b3c7f2dd43f685e4b777d05597c9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c82478a1a094af0816d8980cb428929": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
